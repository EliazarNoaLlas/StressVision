"""
Conversor a TensorFlow Lite - Stress Vision
Optimizaci√≥n y conversi√≥n de modelos para Raspberry Pi 5

Optimizaciones:
- Quantization INT8 (4x reducci√≥n de tama√±o)
- Optimizaci√≥n de operaciones
- Benchmark de latencia

Autor: Gloria S.A.
Fecha: 2024
"""

import tensorflow as tf
import numpy as np
import time
import json
import os
from pathlib import Path


class TFLiteConverter:
    def __init__(self, model_path):
        """
        Inicializa el conversor.
        
        Args:
            model_path: Path al modelo Keras (.keras o .h5)
        """
        self.model_path = model_path
        
        print(f"üì• Cargando modelo desde: {model_path}")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Modelo no encontrado: {model_path}")
        
        self.model = tf.keras.models.load_model(model_path)
        
        print(f"‚úÖ Modelo cargado: {self.model.name}")
        print(f"   ‚Ä¢ Input shape: {self.model.input_shape}")
        print(f"   ‚Ä¢ Output shape: {self.model.output_shape}")
        print(f"   ‚Ä¢ Par√°metros: {self.model.count_params():,}")
    
    def convert_float32(self, output_path='model_float32.tflite'):
        """
        Convierte modelo a TFLite Float32 (sin quantization).
        
        Ventajas:
        - M√°xima precisi√≥n
        - Conversi√≥n simple
        
        Desventajas:
        - Tama√±o grande
        - Latencia alta
        
        Args:
            output_path: Path de salida
            
        Returns:
            output_path: Path al modelo convertido
        """
        print("\n" + "="*80)
        print("üîÑ CONVERSI√ìN A TFLITE FLOAT32")
        print("="*80)
        
        # Configurar converter
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        
        # Sin optimizaciones
        print("   ‚Ä¢ Tipo: Float32")
        print("   ‚Ä¢ Optimizaciones: Ninguna")
        
        # Convertir
        print("\n‚è≥ Convirtiendo...")
        tflite_model = converter.convert()
        
        # Guardar
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        # Estad√≠sticas
        original_size = self.model.count_params() * 4 / (1024**2)  # MB
        tflite_size = len(tflite_model) / (1024**2)  # MB
        
        print(f"\nüìä Resultados:")
        print(f"   ‚Ä¢ Modelo original: {original_size:.2f} MB")
        print(f"   ‚Ä¢ Modelo TFLite: {tflite_size:.2f} MB")
        print(f"   ‚Ä¢ Guardado en: {output_path}")
        print("="*80)
        
        return output_path
    
    def convert_float16(self, output_path='model_float16.tflite'):
        """
        Convierte modelo a TFLite Float16.
        
        Ventajas:
        - Reducci√≥n 2x de tama√±o
        - P√©rdida m√≠nima de precisi√≥n
        - Compatible con la mayor√≠a de dispositivos
        
        Args:
            output_path: Path de salida
            
        Returns:
            output_path: Path al modelo convertido
        """
        print("\n" + "="*80)
        print("üîÑ CONVERSI√ìN A TFLITE FLOAT16")
        print("="*80)
        
        # Configurar converter
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        
        # Optimizaci√≥n Float16
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        
        print("   ‚Ä¢ Tipo: Float16")
        print("   ‚Ä¢ Optimizaciones: DEFAULT")
        print("   ‚Ä¢ Reducci√≥n esperada: ~50%")
        
        # Convertir
        print("\n‚è≥ Convirtiendo...")
        tflite_model = converter.convert()
        
        # Guardar
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        # Estad√≠sticas
        original_size = self.model.count_params() * 4 / (1024**2)
        tflite_size = len(tflite_model) / (1024**2)
        reduction = (1 - tflite_size/original_size) * 100
        
        print(f"\nüìä Resultados:")
        print(f"   ‚Ä¢ Modelo original: {original_size:.2f} MB")
        print(f"   ‚Ä¢ Modelo TFLite: {tflite_size:.2f} MB")
        print(f"   ‚Ä¢ Reducci√≥n: {reduction:.1f}%")
        print(f"   ‚Ä¢ Guardado en: {output_path}")
        print("="*80)
        
        return output_path
    
    def convert_int8(self, representative_dataset_generator=None, 
                     output_path='model_int8.tflite'):
        """
        Convierte modelo a TFLite INT8 con post-training quantization.
        
        Ventajas:
        - Reducci√≥n 4x de tama√±o
        - Inferencia m√°s r√°pida en CPU
        - Menor consumo de energ√≠a
        
        Requiere:
        - Dataset representativo para calibraci√≥n
        
        Args:
            representative_dataset_generator: Generator de datos para calibraci√≥n
            output_path: Path de salida
            
        Returns:
            output_path: Path al modelo convertido
        """
        print("\n" + "="*80)
        print("üîÑ CONVERSI√ìN A TFLITE INT8 (QUANTIZED)")
        print("="*80)
        
        if representative_dataset_generator is None:
            print("‚ö†Ô∏è  No se proporcion√≥ dataset representativo")
            print("   Usando datos sint√©ticos para calibraci√≥n...")
            representative_dataset_generator = self._generate_representative_dataset()
        
        # Configurar converter
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        
        # Optimizaciones
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        # Quantization INT8
        converter.representative_dataset = representative_dataset_generator
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8
        
        print("   ‚Ä¢ Tipo: INT8")
        print("   ‚Ä¢ Optimizaciones: DEFAULT + INT8")
        print("   ‚Ä¢ Input/Output: UINT8")
        print("   ‚Ä¢ Reducci√≥n esperada: ~75%")
        
        # Convertir
        print("\n‚è≥ Convirtiendo (puede tomar varios minutos)...")
        try:
            tflite_model = converter.convert()
        except Exception as e:
            print(f"‚ùå Error en conversi√≥n: {e}")
            print("   Intente con Float16 o revise el modelo")
            return None
        
        # Guardar
        with open(output_path, 'wb') as f:
            f.write(tflite_model)
        
        # Estad√≠sticas
        original_size = self.model.count_params() * 4 / (1024**2)
        tflite_size = len(tflite_model) / (1024**2)
        reduction = (1 - tflite_size/original_size) * 100
        
        print(f"\nüìä Resultados:")
        print(f"   ‚Ä¢ Modelo original: {original_size:.2f} MB")
        print(f"   ‚Ä¢ Modelo TFLite INT8: {tflite_size:.2f} MB")
        print(f"   ‚Ä¢ Reducci√≥n: {reduction:.1f}%")
        print(f"   ‚Ä¢ Guardado en: {output_path}")
        print("="*80)
        
        return output_path
    
    def _generate_representative_dataset(self, num_samples=100):
        """
        Genera dataset representativo sint√©tico para calibraci√≥n.
        
        Args:
            num_samples: N√∫mero de muestras
            
        Returns:
            generator: Generator de datos
        """
        input_shape = self.model.input_shape[1:]  # Sin batch dimension
        
        def representative_dataset_gen():
            for _ in range(num_samples):
                # Generar imagen aleatoria normalizada
                data = np.random.rand(1, *input_shape).astype(np.float32)
                yield [data]
        
        return representative_dataset_gen
    
    def benchmark_latency(self, tflite_path, num_runs=100, num_warmup=10):
        """
        Mide latencia de inferencia del modelo TFLite.
        
        Args:
            tflite_path: Path al modelo TFLite
            num_runs: N√∫mero de ejecuciones para benchmark
            num_warmup: N√∫mero de ejecuciones de calentamiento
            
        Returns:
            stats: Dict con estad√≠sticas de latencia
        """
        print("\n" + "="*80)
        print(f"‚ö° BENCHMARK DE LATENCIA")
        print("="*80)
        print(f"   ‚Ä¢ Modelo: {tflite_path}")
        print(f"   ‚Ä¢ Warmup runs: {num_warmup}")
        print(f"   ‚Ä¢ Benchmark runs: {num_runs}")
        
        # Cargar modelo TFLite
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        # Obtener detalles de input/output
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        print(f"   ‚Ä¢ Input shape: {input_details[0]['shape']}")
        print(f"   ‚Ä¢ Input dtype: {input_details[0]['dtype']}")
        print(f"   ‚Ä¢ Output shape: {output_details[0]['shape']}")
        
        # Preparar input dummy
        input_shape = input_details[0]['shape']
        input_dtype = input_details[0]['dtype']
        
        if input_dtype == np.uint8:
            input_data = np.random.randint(0, 256, input_shape, dtype=np.uint8)
        else:
            input_data = np.random.rand(*input_shape).astype(input_dtype)
        
        # Warmup
        print(f"\n‚è≥ Warmup...")
        for _ in range(num_warmup):
            interpreter.set_tensor(input_details[0]['index'], input_data)
            interpreter.invoke()
        
        # Benchmark
        print(f"‚è≥ Ejecutando benchmark...")
        times = []
        
        for i in range(num_runs):
            start = time.perf_counter()
            interpreter.set_tensor(input_details[0]['index'], input_data)
            interpreter.invoke()
            end = time.perf_counter()
            
            times.append((end - start) * 1000)  # Convert to ms
            
            if (i + 1) % 25 == 0:
                print(f"   Completado: {i + 1}/{num_runs}")
        
        # Calcular estad√≠sticas
        times = np.array(times)
        stats = {
            'mean_ms': float(np.mean(times)),
            'std_ms': float(np.std(times)),
            'min_ms': float(np.min(times)),
            'max_ms': float(np.max(times)),
            'p50_ms': float(np.percentile(times, 50)),
            'p95_ms': float(np.percentile(times, 95)),
            'p99_ms': float(np.percentile(times, 99)),
            'fps_theoretical': float(1000 / np.mean(times))
        }
        
        # Mostrar resultados
        print(f"\nüìä Resultados del Benchmark:")
        print(f"   ‚Ä¢ Latencia promedio: {stats['mean_ms']:.2f} ms")
        print(f"   ‚Ä¢ Desviaci√≥n est√°ndar: {stats['std_ms']:.2f} ms")
        print(f"   ‚Ä¢ M√≠nimo: {stats['min_ms']:.2f} ms")
        print(f"   ‚Ä¢ M√°ximo: {stats['max_ms']:.2f} ms")
        print(f"   ‚Ä¢ P50 (mediana): {stats['p50_ms']:.2f} ms")
        print(f"   ‚Ä¢ P95: {stats['p95_ms']:.2f} ms")
        print(f"   ‚Ä¢ P99: {stats['p99_ms']:.2f} ms")
        print(f"   ‚Ä¢ FPS te√≥rico: {stats['fps_theoretical']:.1f}")
        
        # Verificar KPI (‚â§ 200ms)
        print(f"\nüéØ Verificaci√≥n de KPIs:")
        if stats['mean_ms'] <= 200:
            print(f"   ‚úÖ CUMPLE KPI de latencia (‚â§ 200ms)")
        else:
            print(f"   ‚ùå NO CUMPLE KPI de latencia")
            print(f"   üìù Sugerencias:")
            print(f"      ‚Ä¢ Reducir input size (ej: 120x120)")
            print(f"      ‚Ä¢ Usar arquitectura m√°s ligera")
            print(f"      ‚Ä¢ Considerar Coral USB TPU")
        
        print("="*80)
        
        return stats
    
    def compare_models(self, model_paths, num_runs=100):
        """
        Compara latencia de m√∫ltiples modelos TFLite.
        
        Args:
            model_paths: Lista de paths a modelos TFLite
            num_runs: N√∫mero de ejecuciones para cada modelo
            
        Returns:
            comparison: Dict con comparaci√≥n de modelos
        """
        print("\n" + "="*80)
        print("üìä COMPARACI√ìN DE MODELOS")
        print("="*80)
        
        results = {}
        
        for model_path in model_paths:
            model_name = Path(model_path).stem
            print(f"\nüîç Evaluando: {model_name}")
            
            stats = self.benchmark_latency(model_path, num_runs=num_runs, num_warmup=10)
            
            # Agregar tama√±o del modelo
            size_mb = os.path.getsize(model_path) / (1024**2)
            stats['size_mb'] = size_mb
            
            results[model_name] = stats
        
        # Tabla comparativa
        print("\n" + "="*80)
        print("TABLA COMPARATIVA")
        print("="*80)
        print(f"{'Modelo':<25} {'Tama√±o (MB)':<12} {'Latencia (ms)':<15} {'FPS':<10} {'KPI':<6}")
        print("-"*80)
        
        for name, stats in results.items():
            kpi_status = "‚úÖ" if stats['mean_ms'] <= 200 else "‚ùå"
            print(f"{name:<25} {stats['size_mb']:<12.2f} {stats['mean_ms']:<15.2f} "
                  f"{stats['fps_theoretical']:<10.1f} {kpi_status}")
        
        print("="*80)
        
        return results


def main():
    """Funci√≥n principal."""
    
    print("\n" + "="*80)
    print("üîÑ CONVERSOR A TENSORFLOW LITE - STRESS VISION")
    print("="*80)
    print("\nGloria S.A. - Sistema de Detecci√≥n de Estr√©s Laboral")
    print("Fase 4: Optimizaci√≥n para Raspberry Pi 5\n")
    print("="*80)
    
    # Solicitar modelo
    model_path = input("\nPath al modelo Keras (.keras) [models/best_model.keras]: ").strip()
    if not model_path:
        model_path = "models/best_model.keras"
    
    if not os.path.exists(model_path):
        print(f"\n‚ùå Modelo no encontrado: {model_path}")
        return
    
    # Inicializar conversor
    converter = TFLiteConverter(model_path)
    
    # Seleccionar tipo de conversi√≥n
    print("\n" + "="*80)
    print("TIPOS DE CONVERSI√ìN DISPONIBLES:")
    print("="*80)
    print("1. Float32 (sin optimizaci√≥n)")
    print("   ‚Ä¢ M√°xima precisi√≥n")
    print("   ‚Ä¢ Tama√±o grande")
    print("\n2. Float16 (optimizaci√≥n media)")
    print("   ‚Ä¢ ~50% reducci√≥n de tama√±o")
    print("   ‚Ä¢ P√©rdida m√≠nima de precisi√≥n")
    print("\n3. INT8 Quantized (m√°xima optimizaci√≥n)")
    print("   ‚Ä¢ ~75% reducci√≥n de tama√±o")
    print("   ‚Ä¢ Inferencia m√°s r√°pida")
    print("   ‚Ä¢ ‚ö†Ô∏è Requiere calibraci√≥n")
    print("\n4. Todas las anteriores")
    
    opcion = input("\nSeleccione opci√≥n (1-4) [3]: ").strip()
    if not opcion:
        opcion = '3'
    
    output_dir = "models/tflite"
    os.makedirs(output_dir, exist_ok=True)
    
    model_paths = []
    
    if opcion in ['1', '4']:
        path = converter.convert_float32(f"{output_dir}/model_float32.tflite")
        if path:
            model_paths.append(path)
    
    if opcion in ['2', '4']:
        path = converter.convert_float16(f"{output_dir}/model_float16.tflite")
        if path:
            model_paths.append(path)
    
    if opcion in ['3', '4']:
        path = converter.convert_int8(output_path=f"{output_dir}/model_int8.tflite")
        if path:
            model_paths.append(path)
    
    # Benchmark
    if model_paths:
        print("\n" + "="*80)
        do_benchmark = input("\n¬øRealizar benchmark de latencia? (s/n) [s]: ").strip().lower()
        
        if do_benchmark != 'n':
            num_runs = input("N√∫mero de ejecuciones [100]: ").strip()
            num_runs = int(num_runs) if num_runs else 100
            
            results = converter.compare_models(model_paths, num_runs=num_runs)
            
            # Guardar resultados
            results_path = f"{output_dir}/benchmark_results.json"
            with open(results_path, 'w') as f:
                json.dump(results, f, indent=2)
            
            print(f"\nüíæ Resultados guardados en: {results_path}")
    
    print("\n" + "="*80)
    print("‚úÖ CONVERSI√ìN COMPLETADA")
    print("="*80)
    print(f"\nüìÅ Modelos guardados en: {output_dir}/")
    print("\nüí° Pr√≥ximos pasos:")
    print("   1. Copiar modelo optimizado a Raspberry Pi")
    print("   2. Integrar en sistema de monitoreo")
    print("   3. Probar en producci√≥n")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()


